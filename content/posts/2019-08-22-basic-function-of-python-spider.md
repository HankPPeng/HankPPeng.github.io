---
title: Basic function of Python-Spider
author: ''
date: '2019-08-22'
slug: basic-function-of-python-spider
categories: []
tags:
  - Study
description: ''
externalLink: ''
series: []
---
&emsp;最近重温Python的内容，在我实际的应用中，用的比较多的是 `import...` 和 `def ...`，在记住一些基本的语法规则以后就可以在一些场合比较好地使用Python。近日也突发奇想，用爬虫来抓取某网站的电影数据，之后将数据推送到自己的设备上面，虽然现在也存在着一些App能够做到这种功能，不过自定义程度不够高，希望学习一下能够满足自己的小小愿望。

&emsp;经过自己尝试编写简单的Pyhton-Spider发现，经常用到的需要进行学习的有以下三项：正则表达式`re`，请求`requests`，调用超文本语句`lxml`。以上三项都是Python中的库，能够调取使用，在这里稍稍地整理一下，以防日后忘记用法。

1.正则表达式`re`

&emsp;接触到了正则表达式之后才发现自己以前都太不懂搜索了，正则表达式就是通过一连串繁琐的字符（当然也可以很简单）来匹配文本，这会想起Word里边的‘查找与替换’，不过，以前我都是输入一个特定的字符然后进行另外一个字符的替换，这放在短文本里边是完全没有问题的，与其输入繁琐的正则表达式，倒不如输入特定的字符。可是，自己写爬虫的时候，用正则表达式提取数据就非常高效了，怎么高效只有自己知道，下面解释一个典型正则表达式的意思：

```
\(?0\d{2}[) -]?\d{8}
```

&emsp;这个表达式我看解释的时候就觉得：嗯，原来就是这么回事啊。啃了教程之后回头看，自己写出来可不容易啊...Ok，这个表达式的意思是（我将它进行拆分）：

  * `\(?`表出现0次或1次；
  * `0`匹配0；
  * `\d{2}`表出现两个随机数字（0-9）；
  * `[) -]?`表`)`，`空格`，`-`中任意一个字符出现0次或1次；
  * `\d{8}`表出现8个随机数字。

&emsp;看吧，其实遵循一些规则，要理解正则表达其实也挺简单的。下面我对自己的笔记进行整理：

  * 正则表达式：一种文本模式，通常为普通字符+元字符组成。
  
  * 硬盘下通配符：`?`通配文件名中0或1个字符；`*`通配文件名的0个或多个字符。
  
  * 语法：个人觉得一个正确的正则表达式的语法应该包括字符，运算优先级和匹配规则。这其中的字符是繁琐的，留在后面整理，先对运算优先级和匹配规则做整理。
  
    * 运算优先级：遵循“从左到右，先高后低”的原则，其实就是整个式子是“从左往右”匹配，当遇到高级的运算符的时候先解释高级的再到低级的。下面按照由高到低的顺序排列运算符：
      * `\`：转义符。
      * `(), []`：圆括号和方括号。
      * `*, +, ?, {}`：限定符。
      * `^, $, \任意字符, 任意字符`：位置顺序。
      * `|`：替换，“或”操作符。
      
    * 匹配规则：出现即匹配。这其中`{1}`表出现一次，`{1,2}`表出现1次或2次，`{1,}`表出现1次或以上。
    
    * 字符：正则表达式描述了一种字符串匹配的模式，可以用来检查一个字串是否含有某种字子串、将匹配的子字串替换或提取出符合匹配条件的子字串等。
    
      * 普通字符：包括所有未显式指定为元字符的所有可打印和不可打印的字符，简单地说，也就是没有被指定为特殊意思的字符，这包括所有大写和小写字母、所有数字、所有标点符号和一些其他符，像C里边的`\n`就是指定为`换行`意思的字符。
      
      * 非打印字符：不进行打印的字符，有其特殊的表达含义：
          * `\cx`：匹配由`x`指明的控制字符，如`\cI`匹配一个水平制表符`\t`，`\cJ`匹配一个换行符`\n`,`\cM`匹配一个回车符`\r`等等。
          * `\f`：匹配一个换页符。
          * `\n`：匹配一个换行符。
          * `\r`：匹配一个回车符。
          * `\s`：匹配任何空白字符，包括空格、制表符、换页符等等，等价于 `[\f\n\r\t\v]`。
          * `\S`：匹配任何非空白字符。
          * `\t`：匹配一个制表符。
          * `\v`：匹配一个垂直制表符。
          
      * 特殊字符：在字符串中存在特殊意义，若需显示则需要在前面添加转义字符`\`。
          * `^`：匹配输入字符串的开始位置；若其在方括号中则表示“非”的意思，即不匹配方括号中的字符串。
          * `$`：匹配输入字符串的结尾位置。
          * `( )`：匹配括号中的字符串；**子表达式可以获取供以后使用**。
          * `[ ]`：匹配括号中的字符的任意一个或范围，如`[03]`表匹配0或者3，`[0-3]`表匹配0到3四个数字中的一个。
          * `{ }`：匹配前面字符出现的长度（次数）。
          * `?`：匹配前面子表达式0次或1次。
          * `*`：匹配前面子表达式0次或多次。
          * `+`：匹配前面子表达式1次或多次。
          * `.`：	匹配除换行符`\n`之外的任何单字符。
          * `|`：指明两项之间的一个选择。
          
      上面差不多是主要的字符，还有将以上字符进行分类的，如定位符，限定符等，但是这些分类也离不开上面的字符，所以在这里不多列举。下面将完整的有着特殊含义的字符附上：来源于[菜鸟教程](https://www.runoob.com/regexp/regexp-metachar.html)。
      
      ![元字符](https://github.com/HankPPeng/HankPeng.com/blob/master/images/%E5%85%83%E5%AD%97%E7%AC%A6.png?raw=true)
      
  * 下面总结在Python下怎么使用正则表达式：
    * 需要在导言区导入：`import re`。
    * 具体的语句有：
      * `re.match(pattern, string, flags)`：其中，`pattern`是正则表达式；`string`是需进行匹配的文本或字符串；`flags`是可选参数（最后总结）；这个语句的意思是在`string`的开始位置开始匹配，若不匹配则返回`None`，也就是`pattern`需要与`string`开头一致才会进行匹配。用`.group()`查看匹配结果。
      * `re.search(pattern, string, flags)`：这个语句相对于`re.match()`来说无需开头一致，查找整个文本或字符串。它们之间的区别在于：`re.match`只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回`None`；而`re.search`匹配整个字符串，直到找到一个匹配。
      * `re.sub(pattern, repl, string, count=0, flags=0)`：`repl`是替换的字符串，也可为一个函数；`count`是模式匹配后替换的最大次数，默认 0 表示替换所有的匹配。
      * `re.compile(pattern, flags)`：`pattern`是一个字符串形式的正则表达式。生成一个正则表达式对象，能够供`re.match`和`re.research`调取使用，最大的好处是能够指定要匹配字符串的位置进行匹配。
      * `re.findall(pattern, string, pos, endpos)`：`pos`是可选参数，指定字符串的起始位置，默认为 0；`endpos`是可选参数，指定字符串的结束位置，默认为字符串的长度。这语句是匹配字符串中所有的正则表达式，而`match`和`search`匹配一次后不再匹配。
      * `re.finditer(pattern, string, flags=0)`：表示在字符串中找到正则表达式所匹配的所有子串，并把它们作为一个迭代器返回。
      * `re.split(pattern, string, maxsplit=0, flags=0)`：`maxsplit`是分割次数，`maxsplit=1`为分割一次，默认为0不限次数。这个语句按照匹配的子串将字符串分割后返回列表。
      * `flags`：
          * `re.I`：使匹配对大小写不敏感；
          * `re.L`：做本地化识别（locale-aware）匹配；
          * `re.M`：多行匹配，影响`^`和`$`；
          * `re.S`：使`.`匹配包括换行在内的所有字符；
          * `re.U`：根据Unicode字符集解析字符。这个标志影响 `\w, \W, \b, \B`；
          * `re.X`：该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解。
          
2.请求`requests`

&emsp;`requests`库是Python的第三方库，对于`scrapy`与`requests`之间，`requests`入门容易，自定义程度更高；`scrapy`是一个爬虫框架，需要进行调用各种命令，但更强大，应用更广泛。这里先对`requests`库进行一些整理，之后有时间再去研究一下`scrapy`，然后记录下来。

* Request：让 HTTP 服务人类

&emsp;一般来说，再熟读网站的`robots.txt`文档之后，只要不去触犯文档的禁止抓取的内容，就可以放心使用爬虫。一般的网站都会有爬虫检测，访问次数过多就会禁止IP访问；也有的直接禁止爬虫，因为爬虫会消耗服务器的流量，这样会对网站访问造成一定的影响。但是合理使用爬虫真的节省一定时间，但是最重要的还是有一定的成就感！
    
* 发送请求：`r = requests.get(url, timeout, headers = headerschange)`，这个命令包括了对网站`url`的请求；`timeout`为设置响应的时间（s），也就是说`requests`在设定时间后停止等待响应；还有就是替换`headers`为`headerchanges`来对某些对访问者有限制的网站进行请求。这里`r = `其实就是对`requests.get`的内容进行储存，作为后续提取信息的响应对象。

* 请求状态：`r.status_code`返回请求状态，返回‘200’为成功，‘404’为失败。而`r.raise_for_status()`可以返回网页请求是否成功的状态，若访问状态为‘200’，则返回‘None’；访问状态为‘404’，则返回错误信息。

* 网站编码：`r.encoding`，这个可以返回网站的编码形式，一般都需要对其进行重新编码后获取信息，利用`r.encoding = r.apparent_encoding`来自动选择合适编码形式或者用`r.encoding = ''`指定某一种编码形式。

* 请求头：`r.request.headers`，这个命令查看本地发送给服务器的头部信息；而`r.headers`是响应头，是服务器响应发送给本地的头部信息。根据上面的描述，更改头部信息`headers`可以更改为浏览器或者用户信息，如`headerschange = {'user-agent':'Mozilla/5.0'}`。

* 返回信息：`r.text`，这个命令可以抓取网站的包括html格式在内的信息，然而怎么提取有用的信息则需要对这些内容进行排版分析。`r.content`返回网页的二进制内容。`r.json()`返回`json`格式的内容。

3.提取`lxml`和`bs4`

&emsp;`lxml`是Python的一个解析库，支持HTML和XML的解析，并且支持XPath的解析，在Python开头需要导入`from lxml import etree`；同时经常会提到的另外一个库是`BeautifulSoup`库，导入格式为`from bs4 import BeautifulSoup`，这个库也可以提取，但个人更喜欢`lxml`库来提取信息，因为XPath这种调取方式让人着迷。

a. `lxml`

  * 解析方法：`html = etree.HTML(r.text)`，这个命令运用HTML解析器对所获得的网页内容进行HTML解析，利用`fhtml = etree.tostring(html)`可以补全`html`的结果（html标签）。
  
  * XPath：XPath可以定位一个你想要得到的信息的位置，如获取某个标题，可以利用`title = html.xpath('//*[@id="content"]/span/p[1]/@title')`来提取‘title’的信息。下面对XPath的语法进行总结：
    
    * 表达式：
    
      * `nodename`：选取此节点的所有子节点。
      * `/`：从根节点选取，也就是从第一层目录选取。
      * `//`：从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。（不用一层一层寻找）
      * `@`：选取属性。
      * `[]`：里面填写数字，表示元素的位置。
      
    &emsp;举个例子：
  
```
<?xml version="1.0" encoding="ISO-8859-1"?>

<bookstore class="store">

<book>
  <title lang="eng">Harry Potter</title>
  <price>29.99</price>
</book>

<book>
  <title lang="eng">Learning XML</title>
  <price>39.95</price>
</book>

</bookstore>
```

&emsp;选取节点：选取‘book’的第一个‘title’--‘Harry Potter’：`title = html.xpath('//bookstore[@class="store"]/book[1]/title/text()')`。这样就能提取‘Harry Potter’的文本，其实还有更简单的写法，如`title = html.xpath('//bookstore/book[1]/@title')`，不过一般不省略`id`名，因为`id`名在html中只能唯一，所以能够很好地辨识功能。
  

b. `bs4`

  * 解析方法：`soup = BeautifulSoup(r.text, 'lxml[html.parser]')`
  
  * 查找信息： `soup.select(" "[path])`，一般使用CSS选择器来选择需要获得信息的路径；在规划好信息路径之后，利用`find_all( )`来查找子节点。
  
  * 获取文本：`soup.get_text()`，获取‘tag’中的文本内容。
  
&emsp;9.3填坑完毕，有新想法再添加。哈哈~ (ㄏ￣▽￣)ㄏ